{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEQLNowBwQsg"
      },
      "source": [
        "# Homework: Transfer Learning & Domain Adaptation\n",
        "##  Named Entity Recognition\n",
        "\n",
        "Today we're gonna solve the problem of named entity recognition. Here's what it does in one picture:\n",
        "![img](https://commons.bmstu.wiki/images/0/00/NER1.png)\n",
        "[image source](https://bit.ly/2Pmg7L2)\n",
        "\n",
        "\n",
        "For each word, in a sentence, model should predict a named entity class: _person, organization, location_ or _miscellaneous_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUtpxw8UXUc6"
      },
      "source": [
        "## Data\n",
        "\n",
        "\n",
        "### Source domain testset\n",
        "\n",
        "Our train set consists from texts from different news sources. Therefore as source-domain testset we will use data from [CoNLL-2003 Shared Task](https://github.com/Franck-Dernoncourt/NeuroNER/blob/master/data/conll2003/en). More information about the task can be found [here](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
        "\n",
        "### Target domain (in-domain) data\n",
        "\n",
        "As target-domain data we will use data from [WNUT17 Emerging and Rare entity recognition task](http://noisy-text.github.io/2017/emerging-rare-entities.html). This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. The data were mined from mined from Twitter, Reddit,\n",
        "YouTube and StackExchange. Results of different competitors of the task were published [here](https://noisy-text.github.io/2017/pdf/WNUT18.pdf).\n",
        "\n",
        "### Named entity classes\n",
        "\n",
        "* PER - _person_: names of people (e.g. Alexander S. Pushkin)\n",
        "* ORG - _organization_: names of corporations (e.g. Yandex), names of non-profit organizations (e.g. UNICEF)\n",
        "Google).\n",
        "* LOC - _location_ : e.g. Russia\n",
        "* MISC - _miscellaneous_ : other named entities including names of products (e.g. iPhone) and creative works (e.g. Bohemian Rhapsody)\n",
        "\n",
        "### Evaluation metrics\n",
        "\n",
        "As evaluation metrics we will F1 measure on exact matched NEs. It means that partially overlapped enitities of same class are considered as mismatch.\n",
        "For example, LOC entities below is partially overlapped. And it is a mismatch:\n",
        "\n",
        "__O, B-LOC, I-LOC, O__\n",
        "\n",
        "__O, B-LOC, I-LOC, I-LOC__\n",
        "\n",
        "Details can be found in the code of _conlleval.py_\n",
        "\n",
        "### Data format\n",
        "\n",
        "The format of all dataset follows popular [IOB format](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)). The B- prefix before a tag indicates that the tag is the beginning of a chunk, and an I- prefix before a tag indicates that the tag is inside a chunk. The B- tag is used only when a tag is followed by a tag of the same type without O tokens between them. An O tag indicates that a token belongs to no chunk.\n",
        "\n",
        "The named entity labels include:\n",
        "* __B-LOC__ - location - first token\n",
        "* __I-LOC__ - location - subsequent tokens\n",
        "* __B-ORG__ - organization - first token\n",
        "* __O__ - not a named entity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3zNgCNWXUc7"
      },
      "source": [
        "### Part 1: dataset exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC7HKW7XXUc8"
      },
      "source": [
        "To load datasets we will use `huggingface/datasets` library:\n",
        "\n",
        "https://huggingface.co/docs/datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i27ShrE6j7DV"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tbsMncqXXUc8"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import typing as tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeWSQSvHXUc-"
      },
      "outputs": [],
      "source": [
        "conll = datasets.load_dataset(\"conll2003\")\n",
        "wnut = datasets.load_dataset(\"wnut_17\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll_test = conll[\"test\"]\n",
        "conll_train = conll[\"train\"]\n",
        "\n",
        "wnut_test = wnut[\"train\"]\n",
        "wnut_train = wnut[\"test\"]"
      ],
      "metadata": {
        "id": "xkjhlAs0Joqe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-903NG1XUc-"
      },
      "source": [
        "This datasets is a actually `DatasetDict`s.\n",
        "\n",
        "To see class hierarchy we can use `getmro` function from `inspect` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LUJlXckeXUc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f7eff5-e025-419d-9b3c-c4064535212d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(datasets.dataset_dict.DatasetDict, dict, object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import inspect\n",
        "inspect.getmro(type(conll))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "djrjPjDCXUc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d456a348-549d-4fe8-ec44-86d8c17cd5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train', 'validation', 'test']) dict_keys(['train', 'validation', 'test'])\n"
          ]
        }
      ],
      "source": [
        "print(conll.keys(), wnut.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpqpC7YwXUdB"
      },
      "source": [
        "Let's have a look at content of test part CONLL dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "phgQGXzyXUdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ecb4b9-c143-418f-f401-10cb67fcf4dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
              " 'id': Value(dtype='string', id=None),\n",
              " 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None),\n",
              " 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
              " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "conll[\"test\"].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B5QHYVYGXUdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ff22fd-1532-4c44-89e6-71c8351bdfe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10252791, 3454)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "conll[\"test\"].dataset_size, len(conll[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkVoMzQiXUdB"
      },
      "source": [
        "Let's have a look at single example in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3NRhD0IaXUdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e13f9e-3d43-424e-8603-43cc82b9b1ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chunk_tags': [11, 13, 11, 12, 12],\n",
              " 'id': '50',\n",
              " 'ner_tags': [0, 0, 0, 7, 8],\n",
              " 'pos_tags': [24, 15, 12, 22, 22],\n",
              " 'tokens': ['Results', 'of', 'the', 'World', 'Cup']}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "conll[\"test\"][50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ftIaOy9XUdC"
      },
      "source": [
        "There is a fancy visualizer in `spacy` nlp library we can adapt for custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zx0RpsevXUdC"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4GCGNz8-XUdD"
      },
      "outputs": [],
      "source": [
        "def get_ner_index(index, names):\n",
        "    return globals()[names].features[\"ner_tags\"].feature.names[index]\n",
        "\n",
        "def ner_render(dicts: str, tokens: tp.Sequence[str], ner_tags: tp.Sequence[str], title: tp.Optional[str] = None, **kwargs):\n",
        "    pos = 0\n",
        "    ents = []\n",
        "    for word, tag in zip(tokens, ner_tags):\n",
        "        tag = get_ner_index(tag, dicts)\n",
        "        if tag.startswith('B'):\n",
        "            ents.append({\n",
        "                \"start\": pos,\n",
        "                \"end\": pos + len(word),\n",
        "                \"label\": tag.split(\"-\")[1]\n",
        "            })\n",
        "        elif tag.startswith('I'):\n",
        "            ents[-1][\"end\"] = pos + len(word)\n",
        "        pos += (len(word) + 1)\n",
        "    displacy.render({\n",
        "        \"text\": \" \".join(tokens),\n",
        "        \"ents\": ents,\n",
        "        \"title\": title\n",
        "    }, style=\"ent\", manual=True, jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZwmpUouXUdD"
      },
      "source": [
        "Voilà!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "7qhPerOuXUdD",
        "outputId": "ea35d41e-9aa6-44e9-d2fa-944399468c8a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Results of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    World Cup\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">League duties restricted the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Barbarians\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " ' selectorial options but they still boast 13 internationals including \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    England\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " full-back \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tim Stimpson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " and recalled wing \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tony Underwood\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " , plus \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    All Black\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " forwards \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ian Jones\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Norm Hewitt\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " .</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ner_render('conll_train', **conll_test[50])\n",
        "ner_render('conll_train', **conll_test[200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUuIbsxJXUdE"
      },
      "source": [
        "Let's compare CONLL and WNUT named entity tags. We need to count each type of tags in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ps7igM5MXUdE"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "conll_tag_counts = Counter()\n",
        "wnut_tag_counts = Counter()\n",
        "\n",
        "for tags in conll_test[\"ner_tags\"]:\n",
        "  conll_tag_counts.update(tags)\n",
        "\n",
        "for tags in wnut_test[\"ner_tags\"]:\n",
        "  wnut_tag_counts.update(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "inm4kYzrXUdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c795f664-568f-4a3e-b42b-d7816be1b7f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 59570,\n",
              "         1: 221,\n",
              "         2: 46,\n",
              "         3: 140,\n",
              "         4: 206,\n",
              "         5: 264,\n",
              "         6: 150,\n",
              "         7: 548,\n",
              "         8: 245,\n",
              "         9: 660,\n",
              "         10: 335,\n",
              "         11: 142,\n",
              "         12: 203})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "wnut_tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "swb175lVXUdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53afaa6-796c-4a47-9b89-7fd85ec747f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 38323,\n",
              "         1: 1617,\n",
              "         2: 1156,\n",
              "         3: 1661,\n",
              "         4: 835,\n",
              "         5: 1668,\n",
              "         6: 257,\n",
              "         7: 702,\n",
              "         8: 216})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "conll_tag_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YOzShmAXUdF"
      },
      "source": [
        "Thus, WNUT and CONLL have different set of NE tags (labels)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGou3EoXUdF"
      },
      "source": [
        "From WNUT description:\n",
        "\n",
        "**person** –  Names  of  people  (e.g.Virginia Wade).   Don’t mark people that don’t have their own name.  Include punctuation in the middle ofnames.  Fictional people can be included, as long as they’re referred to by name (e.g.Harry Potter).\n",
        "\n",
        "**location** –  Names  that  are  locations  (e.g. France).    Don’t  mark  locations  that  don’t  have their own name.  Include punctuation in the middle of names. Fictional locations can be included, as long as they’re referred to by name (e.g.Hogwarts)\n",
        "\n",
        "**corporation** –  Names  of  corporations  (e.g.Google).   Don’t  mark  locations  that  don’t  have their own name. Include punctuation in the middle of names\n",
        "\n",
        "**product** –  Name  of  products  (e.g. iPhone). Don’t  mark  products  that  don’t  have  their  own name. Include punctuation in the middle of names. Fictional  products  can  be  included,  as  long  as they’re referred to by name (e.g.Everlasting Gobstopper).  It’s got to be something you can touch, and it’s got to be the official name.\n",
        "\n",
        "**creative-work** –  Names  of  creative  works (e.g.Bohemian Rhapsody). Include punctuation inthe middle of names. The work should be created by a human, and referred to by its specific name.\n",
        "\n",
        "**group** – Names of groups (e.g.Nirvana, SanDiego  Padres). Don’t  mark  groups  that  don’t have a specific, unique name, or companies (which should be marked corporation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaEj12LGXUdF"
      },
      "source": [
        "We can match CONLL and WNUT labels using next rules:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll_test.features[\"ner_tags\"].feature.names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNV3yJiQKAzg",
        "outputId": "bb950559-c5d6-4a48-b64b-67b14734eb1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnut_test.features[\"ner_tags\"].feature.names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-YCN8hLGJD",
        "outputId": "e575910e-31e6-433b-e9d2-a082a37c354d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-corporation',\n",
              " 'I-corporation',\n",
              " 'B-creative-work',\n",
              " 'I-creative-work',\n",
              " 'B-group',\n",
              " 'I-group',\n",
              " 'B-location',\n",
              " 'I-location',\n",
              " 'B-person',\n",
              " 'I-person',\n",
              " 'B-product',\n",
              " 'I-product']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xWKYdQK2XUdF"
      },
      "outputs": [],
      "source": [
        "label_mapping = {\n",
        "    0: 0, # 'O': 'O',\n",
        "    7: 5, # 'B-location': 'B-LOC',\n",
        "    8: 6, # 'I-location': 'I-LOC',\n",
        "    5: 3, # 'B-group': 'B-ORG',\n",
        "    1: 3, # 'B-corporation': 'B-ORG',\n",
        "    9: 1, # 'B-person': 'B-PER',\n",
        "    3: 7, # 'B-creative-work': 'B-MISC',\n",
        "    11: 7, # 'B-product': 'B-MISC',\n",
        "    10: 2, # 'I-person': 'I-PER',\n",
        "    4: 8, # 'I-creative-work': 'I-MISC',\n",
        "    2: 4, # 'I-corporation': 'I-ORG',\n",
        "    6: 4, # 'I-group': 'I-ORG',\n",
        "    12: 8 # 'I-product': 'I-MISC'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Oh4ngTTiXUdG"
      },
      "outputs": [],
      "source": [
        "def convert_label_sequence(example: tp.Dict[str, tp.Any], label_mapping: tp.Dict[str, str]) -> tp.Dict[str, tp.Any]:\n",
        "    converted_example = dict(**example)\n",
        "    converted_example[\"ner_tags\"] = [label_mapping[label] for label in example[\"ner_tags\"]]\n",
        "    return converted_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjMyI7FKXUdG"
      },
      "outputs": [],
      "source": [
        "converted_wnut = wnut.map(lambda x: convert_label_sequence(x, label_mapping))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_wnut_train = converted_wnut[\"train\"]\n",
        "converted_wnut_test = converted_wnut[\"test\"]"
      ],
      "metadata": {
        "id": "d7ZOzJnyNPob"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_wnut[\"train\"].features[\"ner_tags\"].feature.names = conll_train.features[\"ner_tags\"].feature.names\n",
        "converted_wnut[\"test\"].features[\"ner_tags\"].feature.names = conll_test.features[\"ner_tags\"].feature.names\n",
        "\n",
        "converted_wnut[\"train\"].features[\"ner_tags\"].feature.num_classes = conll_train.features[\"ner_tags\"].feature.num_classes\n",
        "converted_wnut[\"test\"].features[\"ner_tags\"].feature.num_classes = conll_test.features[\"ner_tags\"].feature.num_classes"
      ],
      "metadata": {
        "id": "YCElrDMndqB8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz_W-7wbXUdG"
      },
      "source": [
        "**Before:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZV3B2QvnXUdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4dc110e3-0f46-4845-8e0f-718c329f0a33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@paulwalk It 's the view from where I 'm living for two weeks . \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Empire State Building\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">location</span>\n",
              "</mark>\n",
              " = \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ESB\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">location</span>\n",
              "</mark>\n",
              " . Pretty bad storm here last evening .</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">From Green Newsfeed : \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AHFA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">group</span>\n",
              "</mark>\n",
              " extends deadline for Sage Award to Nov . 5 http://tinyurl.com/24agj38</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Pxleyes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">corporation</span>\n",
              "</mark>\n",
              " Top 50 Photography Contest Pictures of August 2010 ... http://bit.ly/bgCyZ0 #photography</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ner_render(\"wnut_train\", **wnut[\"train\"][0])\n",
        "ner_render(\"wnut_train\", **wnut[\"train\"][1])\n",
        "ner_render(\"wnut_train\", **wnut[\"train\"][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZYEwm8hXUdG"
      },
      "source": [
        "**After label mapping:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vN16z3dtXUdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "70473054-7f7b-45b9-92ff-3c5c8fa899f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@paulwalk It 's the view from where I 'm living for two weeks . \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Empire State Building\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " = \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ESB\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " . Pretty bad storm here last evening .</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">From Green Newsfeed : \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AHFA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " extends deadline for Sage Award to Nov . 5 http://tinyurl.com/24agj38</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Pxleyes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " Top 50 Photography Contest Pictures of August 2010 ... http://bit.ly/bgCyZ0 #photography</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ner_render(\"converted_wnut_train\", **converted_wnut[\"train\"][0])\n",
        "ner_render(\"converted_wnut_train\", **converted_wnut[\"train\"][1])\n",
        "ner_render(\"converted_wnut_train\", **converted_wnut[\"train\"][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eX3utU6XUdH"
      },
      "source": [
        "To visualize lexical differences between WNUT and CONLL let's use `scattertext` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LQnGKtOMXUdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6f876c85-91d7-4e6b-f675-cbf094ecdd92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:98% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import IFrame\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqKJBD7NXUdH"
      },
      "outputs": [],
      "source": [
        "!pip3 install scattertext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQyHJcnhXUdI"
      },
      "outputs": [],
      "source": [
        "import scattertext as st\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzNqgbTPXUdI"
      },
      "outputs": [],
      "source": [
        "conll_df = pd.DataFrame([{\"text\": \" \".join(example[\"tokens\"]), \"ner_tags\": example[\"ner_tags\"], \"dataset\": \"conll\"} for example in conll[\"train\"]])\n",
        "wnut_df = pd.DataFrame([{\"text\": \" \".join(example[\"tokens\"]), \"ner_tags\": example[\"ner_tags\"], \"dataset\": \"wnut\"} for example in converted_wnut[\"train\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeMUPeeKXUdI"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([conll_df, wnut_df]).assign(\n",
        "    parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
        ")\n",
        "\n",
        "corpus = st.CorpusFromParsedDocuments(\n",
        "    df, category_col='dataset', parsed_col='parse'\n",
        ").build().get_unigram_corpus().compact(st.AssociationCompactor(2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1-1ue1RXUdI"
      },
      "outputs": [],
      "source": [
        "html = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category='conll', category_name='CONLL', not_category_name='WNUT',\n",
        "    minimum_term_frequency=0, pmi_threshold_coefficient=0,\n",
        "    width_in_pixels=1000,\n",
        "    transform=st.Scalers.dense_rank\n",
        ")\n",
        "\n",
        "with open(\"difference.html\", \"w\") as outf:\n",
        "    print(html, file=outf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDSSNZl4XUdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a206fe0c-f8a1-48b1-d8a0-a35e5e9c4e40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fe97cc44f10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1200\"\n",
              "            height=\"1000\"\n",
              "            src=\"difference.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "IFrame(\"difference.html\", width=1200, height=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mu1-6P2XUdJ"
      },
      "source": [
        "### Part 2: BERT sequence labeling recap (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIDPkgHVXUdJ"
      },
      "source": [
        "https://huggingface.co/transformers/task_summary.html#named-entity-recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1SPINPXUdJ"
      },
      "source": [
        "The sequence labeling task is a degenerate case of the seq2seq task: we need to map a sequence of words to a sequence of labels (tags) of **the same length**.\n",
        "\n",
        "In case of BERT we want to get a vector of probabilities of labels for each input token. The simplest way to make it is just feed output token embeddings to Linear layer.\n",
        "\n",
        "`transformers` class `BertForTokenClassification` works just like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea1j2WibXUdJ"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "Image from FastAI article: https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayhOelawXUdJ"
      },
      "source": [
        "#### 2.1 Load pre-trained BERT-based sequence tagger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "3Ly-znNDyosp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "NXEVsygKXUdJ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "\n",
        "name = \"dslim/bert-base-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDZ80ecRXUdK"
      },
      "source": [
        "Let's have a look at `BertForTokenClassification` class hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dgZea0lcXUdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ef7271-65ed-4252-9fa2-0437b084dbb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(transformers.models.bert.modeling_bert.BertForTokenClassification,\n",
              " transformers.models.bert.modeling_bert.BertPreTrainedModel,\n",
              " transformers.modeling_utils.PreTrainedModel,\n",
              " torch.nn.modules.module.Module,\n",
              " transformers.modeling_utils.ModuleUtilsMixin,\n",
              " transformers.generation_utils.GenerationMixin,\n",
              " transformers.utils.hub.PushToHubMixin,\n",
              " object)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "inspect.getmro(type(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoVH8REhXUdK"
      },
      "source": [
        "In `transformers.BertPreTrainedModel` there is an important attribute `config`. Let's have a look. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "scKVUW31XUdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3370fd-d39e-4a5b-ec0f-ae7710f5fbeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
              "  \"_num_labels\": 9,\n",
              "  \"architectures\": [\n",
              "    \"BertForTokenClassification\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"O\",\n",
              "    \"1\": \"B-MISC\",\n",
              "    \"2\": \"I-MISC\",\n",
              "    \"3\": \"B-PER\",\n",
              "    \"4\": \"I-PER\",\n",
              "    \"5\": \"B-ORG\",\n",
              "    \"6\": \"I-ORG\",\n",
              "    \"7\": \"B-LOC\",\n",
              "    \"8\": \"I-LOC\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"B-LOC\": 7,\n",
              "    \"B-MISC\": 1,\n",
              "    \"B-ORG\": 5,\n",
              "    \"B-PER\": 3,\n",
              "    \"I-LOC\": 8,\n",
              "    \"I-MISC\": 2,\n",
              "    \"I-ORG\": 6,\n",
              "    \"I-PER\": 4,\n",
              "    \"O\": 0\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.18.0\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 28996\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCrUTDbAXUdK"
      },
      "source": [
        "As mentioned above `BertForTokenClassification` = BERT + linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bX69wTNMXUdK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5079fdda-52e7-4401-c54a-cb751ca889fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=9, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "model.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to change order of `ner_tags`, because in ConLL dataset there are another."
      ],
      "metadata": {
        "id": "82H2NJuafPNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label = {key: value for key, value in enumerate(conll[\"test\"].features[\"ner_tags\"].feature.names)}\n",
        "model.config.label2id = {value: key for key, value in enumerate(conll[\"test\"].features[\"ner_tags\"].feature.names)}"
      ],
      "metadata": {
        "id": "ZX3z3pT4j6xI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjjQRvKfXUdL"
      },
      "source": [
        "Let's explore one example from dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nRvimZYwXUdL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2181a03f-f59c-4dad-c665-9265c3bb1032"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Takuya Takagi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " scored the winner in the 88th minute , rising to head a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hiroshige Yanagimoto\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " cross towards the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Syrian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " goal which goalkeeper \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Salem Bitar\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " appeared to have covered but then allowed to slip into the net .</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ner_render('conll_test', **conll[\"test\"][10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9rum4yZXUdL"
      },
      "source": [
        "Tokenizer call splits sequence of words to sequnce of word pieces (bpe units) and maps it to token ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XOrBs2BpXUdL"
      },
      "outputs": [],
      "source": [
        "encoded = tokenizer(conll[\"test\"][10][\"tokens\"], is_split_into_words=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SMvz1KCrXUdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352a86c9-66da-420b-f76f-2aaee0bd8fca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 22515, 4786, 2315, 22515, 1968, 5389, 2297, 1103, 2981, 1107, 1103, 5385, 1582, 2517, 117, 4703, 1106, 1246, 170, 8790, 5864, 3031, 2176, 14932, 27547, 12610, 2771, 2019, 1103, 8697, 2273, 1134, 10159, 10293, 27400, 1813, 1691, 1106, 1138, 2262, 1133, 1173, 2148, 1106, 7324, 1154, 1103, 5795, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bc-e-zXUdL"
      },
      "source": [
        "Here:\n",
        "\n",
        "`input_ids` - indices of input sequence tokens in the vocabulary\n",
        "\n",
        "`token_type_ids` - segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]: 0 corresponds to a sentence A token, 1 corresponds to a sentence B token\n",
        "\n",
        "`attention_mask` - mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEe3AJgHXUdM"
      },
      "source": [
        "As we discussed above in sequence labeling task output sequence length should be equal to input sequence length.\n",
        "\n",
        "Tokenizer can split single world to multiple pieces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ap3eLdq7XUdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c62d894-6f20-4a60-d43a-4455e891d49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence:\n",
            "---\n",
            "Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .\n",
            "\n",
            "Tokenized sentence:\n",
            "---\n",
            "[CLS] Ta ##ku ##ya Ta ##ka ##gi scored the winner in the 88 ##th minute , rising to head a Hi ##ros ##hi ##ge Yan ##agi ##moto cross towards the Syrian goal which goalkeeper Salem Bit ##ar appeared to have covered but then allowed to slip into the net . [SEP]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Original sentence:\\n---\\n{' '.join(conll['test'][10]['tokens'])}\")\n",
        "\n",
        "tokenized = \" \".join([tokenizer.convert_ids_to_tokens(id_) for id_ in encoded[\"input_ids\"]])\n",
        "print(f\"\\nTokenized sentence:\\n---\\n{tokenized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kkr9b6YXUdM"
      },
      "source": [
        "For example named entity `Takuya Takagi` transforms into `Ta ##ku ##ya Ta ##ka ##gi`. We need to preserve invariant: input sequence length = output sequence length. So sequence labels (tags) should be tokenized too.\n",
        "\n",
        "\n",
        "```\n",
        "Takuya Takagi -> Ta    ##ku   ##ya   Ta     ##ka    ##gi\n",
        "   |     |        |     |       |     |      |        |\n",
        "B-PER  I-PER    B-PER  I-PER  I-PER  I-PER  I-PER   I-PER \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4963N5DXUdM"
      },
      "source": [
        "Let's write function for it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "ARyOnFPtzLAm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "CWJORHffXUdM"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_tags(example: tp.Dict[str, tp.Any],\n",
        "                               model,\n",
        "                               tokenizer: transformers.BertTokenizer,\n",
        "                               label2id: tp.Dict[str, int],\n",
        "                               tokenizer_params={}) -> tp.Dict[str, tp.Any]:\n",
        "    # function to split each pair of word-token to same number of pieces.\n",
        "    encoded = tokenizer(example[\"tokens\"], is_split_into_words=True, **tokenizer_params)\n",
        "    encoded.update(example)\n",
        "\n",
        "    word_ids = encoded.word_ids()\n",
        "    text_labels = []\n",
        "    for i in range(1, len(word_ids) - 1):\n",
        "        label = example[\"ner_tags\"][word_ids[i]]\n",
        "        if isinstance(label, int):\n",
        "            label = model.config.id2label[label]\n",
        "        if label == \"O\" or word_ids[i - 1] != word_ids[i]:\n",
        "            text_labels.append(label)\n",
        "        else:\n",
        "            text_labels.append(\"I\" + label[1:])\n",
        "    text_labels = [\"O\"] + text_labels + [\"O\"]\n",
        "\n",
        "    for key in ['input_ids', 'attention_mask', 'token_type_ids']:\n",
        "        encoded[key] = torch.tensor(encoded[key])\n",
        "\n",
        "    encoded['labels'] = [label2id[label] for label in text_labels]\n",
        "    encoded['text_labels'] = text_labels\n",
        "    \n",
        "    assert len(encoded['labels']) == len(encoded[\"input_ids\"])\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nk6tAPlGXUdM"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"His name is Jerry Abrahamson\"\n",
        "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\"]}\n",
        "test_result = tokenize_and_preserve_tags(test_example, model, tokenizer, model.config.label2id)\n",
        "\n",
        "assert tokenizer.decode(test_result['input_ids']) == '[CLS] His name is Jerry Abrahamson [SEP]'\n",
        "\n",
        "                                     #CLS     His  name is    Jerry    Abraham   ##son      SEP\n",
        "assert test_result['text_labels'] == [\"O\"] + [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\",  \"I-PER\"] + [\"O\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWKtAAK9XUdN"
      },
      "outputs": [],
      "source": [
        "conll = conll.map(lambda x: tokenize_and_preserve_tags(x, model, tokenizer, model.config.label2id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlQ2ZpXHXUdN"
      },
      "outputs": [],
      "source": [
        "wnut = converted_wnut\n",
        "wnut = wnut.map(lambda x: tokenize_and_preserve_tags(x, model, tokenizer, model.config.label2id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhEcCu1sXUdN"
      },
      "source": [
        "Next step is convert all numpy.arrays to torch.tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "korHfrXGXUdN"
      },
      "outputs": [],
      "source": [
        "conll.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
        "wnut.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], output_all_columns=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "zF8-xG0PXUdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1ca818-c06d-48f4-965d-3930e3eaa7e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'chunk_tags': [11, 0, 11, 12, 12, 12],\n",
              " 'id': '2',\n",
              " 'input_ids': tensor([  101, 18589,   118, 19016,  2249,   117,  1244,  4699, 14832,  1820,\n",
              "           118,  1367,   118,  5037,   102]),\n",
              " 'labels': tensor([0, 5, 6, 6, 6, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0]),\n",
              " 'ner_tags': [5, 0, 5, 6, 6, 0],\n",
              " 'pos_tags': [22, 6, 22, 22, 23, 11],\n",
              " 'text_labels': ['O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "conll[\"test\"][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua62iurEXUdO"
      },
      "source": [
        "To use `transformers.Dataset` with `torch.DataLoader` we need a custom function to pad sequences and make batches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "KOnUOSj37QcI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "e-DvhnB5bkB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "c1RFYo1yXUdO"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "class PadSequence:\n",
        "    def __init__(self, padded_columns, device='cuda'):\n",
        "        self.padded_columns = set(padded_columns)\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        padded_batch = defaultdict(list)\n",
        "        for example in batch:\n",
        "            for key, tensor in example.items():\n",
        "                padded_batch[key].append(tensor)\n",
        "                \n",
        "        for key, val in padded_batch.items():\n",
        "            if key in self.padded_columns:\n",
        "                padded_batch[key] = torch.nn.utils.rnn.pad_sequence(val, batch_first=True).to(self.device)\n",
        "        return padded_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "nQpUUIxKXUdO"
      },
      "outputs": [],
      "source": [
        "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], batch_size=4, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qVNReXXUdO"
      },
      "source": [
        "Let's test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "QtaYu_NHXUdO"
      },
      "outputs": [],
      "source": [
        "test_batch = next(iter(conll_test_dataloader))\n",
        "model_output = model(input_ids=test_batch[\"input_ids\"],\n",
        "                     token_type_ids=test_batch[\"token_type_ids\"],\n",
        "                     attention_mask=test_batch[\"attention_mask\"],\n",
        "                     labels=test_batch[\"labels\"], return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jDfMuZXUdP"
      },
      "source": [
        "Model output contains only `loss` and `logits`. We need to write simple wrapper to convert raw logits to token sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "A4WR6TOaXUdP"
      },
      "outputs": [],
      "source": [
        "class NamedEntityPredictor:\n",
        "    def __init__(self,\n",
        "                 model: transformers.BertForTokenClassification,\n",
        "                 tokenizer: transformers.BertTokenizer,\n",
        "                 id2label: tp.Optional[tp.Dict[str, int]] = None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.id2label = model.config.id2label if id2label is None else id2label\n",
        "    \n",
        "    def predict(self, batch: tp.Dict[str, tp.Any]):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            model_output = self.model(input_ids=batch[\"input_ids\"],\n",
        "                                      token_type_ids=batch[\"token_type_ids\"],\n",
        "                                      attention_mask=batch[\"attention_mask\"],\n",
        "                                      labels=batch[\"labels\"],\n",
        "                                      return_dict=True)\n",
        "        indices = torch.argmax(model_output.logits, axis=2)\n",
        "        indices = indices.detach().cpu().numpy()\n",
        "\n",
        "        attention_mask = batch[\"attention_mask\"].cpu().numpy()\n",
        "        batch_size = len(batch[\"input_ids\"])\n",
        "        predicted_labels = []\n",
        "        for i in range(batch_size):\n",
        "            predicted_labels.append([self.id2label[id_] for id_ in indices[i][attention_mask[i] == 1]])\n",
        "            \n",
        "        return {\n",
        "            \"predicted_labels\": predicted_labels,\n",
        "            \"loss\": model_output.loss,\n",
        "            \"logits\": model_output.logits\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "rS22kbJyXUdP"
      },
      "outputs": [],
      "source": [
        "ner = NamedEntityPredictor(model, tokenizer)\n",
        "test_prediction = ner.predict(test_batch)\n",
        "assert test_prediction['predicted_labels'][2] == list(test_batch[\"text_labels\"][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "RDeLD2kiXUdP"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsI01QtBXUdP"
      },
      "source": [
        "Let's measure quality of NER for in-domain and out-domain testset.\n",
        "\n",
        "To compare models we can use F1 measure. There is a great package `seqeval` to make such quality measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i66_942dXUdQ"
      },
      "outputs": [],
      "source": [
        "!pip3 install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "5M13ZOHZXUdQ"
      },
      "outputs": [],
      "source": [
        "conll_test_dataloader = torch.utils.data.DataLoader(conll[\"test\"], batch_size=16, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))\n",
        "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], batch_size=16, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY2dC1RmXUdQ"
      },
      "outputs": [],
      "source": [
        "ner = NamedEntityPredictor(model, tokenizer)\n",
        "predicted_labels = {\"wnut_test\": [], \"conll_test\": []}\n",
        "\n",
        "for batch in tqdm.tqdm_notebook(conll_test_dataloader):\n",
        "    predicted_labels[\"conll_test\"].extend(ner.predict(batch)[\"predicted_labels\"])\n",
        "    \n",
        "for batch in tqdm.tqdm_notebook(wnut_test_dataloader):\n",
        "    predicted_labels[\"wnut_test\"].extend(ner.predict(batch)[\"predicted_labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "c1epsWOwXUdQ"
      },
      "outputs": [],
      "source": [
        "import seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "0LvaPwRJIHoU"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "l-rFC2o4XUdQ"
      },
      "outputs": [],
      "source": [
        "conll_report = classification_report(y_true=[list(example[\"text_labels\"]) for example in conll[\"test\"]],\n",
        "                                                     y_pred=predicted_labels[\"conll_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "PVNu22h7XUdQ"
      },
      "outputs": [],
      "source": [
        "wnut_report = classification_report(y_true=[list(example[\"text_labels\"]) for example in wnut[\"test\"]],\n",
        "                                                    y_pred=predicted_labels[\"wnut_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "vqgU3FvmXUdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60aaff9e-fea9-4e53-d63b-1aeab44dd69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONLL:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.75      0.84      0.79      1668\n",
            "        MISC       0.58      0.70      0.63       702\n",
            "         ORG       0.71      0.82      0.76      1661\n",
            "         PER       0.36      0.58      0.45      1617\n",
            "\n",
            "   micro avg       0.58      0.74      0.65      5648\n",
            "   macro avg       0.60      0.74      0.66      5648\n",
            "weighted avg       0.61      0.74      0.66      5648\n",
            "\n",
            "WNUT:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.45      0.51      0.48       150\n",
            "        MISC       0.17      0.22      0.19       269\n",
            "         ORG       0.16      0.30      0.21       231\n",
            "         PER       0.30      0.30      0.30       429\n",
            "\n",
            "   micro avg       0.24      0.31      0.27      1079\n",
            "   macro avg       0.27      0.33      0.29      1079\n",
            "weighted avg       0.26      0.31      0.28      1079\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"CONLL:\\n {conll_report}\")\n",
        "print(f\"WNUT:\\n {wnut_report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GUceHsrXUdR"
      },
      "source": [
        "As we can see WNUT model perfomance is poor. Let's test hypothesis that for CONLL-like inputs NER quality will be higher.\n",
        "\n",
        "To make this we will train a simple log-regression to classify sentences between two datasets based on their sentence embeddings.\n",
        "\n",
        "The sentence embeddings can be obtained from [CLS] token embedding.\n",
        "\n",
        "For classification we will train vanilla sklearn log-regression. Just like on picture below, but we will use ordinary BERT for sentence embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-xj4o-YXUdR"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCLfXnyeXUdR"
      },
      "source": [
        "Picture from great [Jay Alammar's blog post](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4xBpbPhXUdR"
      },
      "outputs": [],
      "source": [
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "d2lD1MPvXUdR"
      },
      "outputs": [],
      "source": [
        "def get_sentence_embeddings(model, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        return model.bert(input_ids=batch[\"input_ids\"],\n",
        "                          token_type_ids=batch[\"token_type_ids\"],\n",
        "                          attention_mask=batch[\"attention_mask\"],\n",
        "                          return_dict=True)[\"last_hidden_state\"].cpu().numpy()[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YULTOzenXUdR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "conll_train_dataloader = torch.utils.data.DataLoader(conll[\"train\"], batch_size=32, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))\n",
        "wnut_train_dataloader = torch.utils.data.DataLoader(wnut[\"train\"], batch_size=32, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))\n",
        "\n",
        "for batch in tqdm.tqdm_notebook(conll_train_dataloader):\n",
        "    X.append(get_sentence_embeddings(model, batch))\n",
        "    Y.extend([0] * len(batch[\"input_ids\"]))\n",
        "    \n",
        "for batch in tqdm.tqdm_notebook(wnut_train_dataloader):\n",
        "    X.append(get_sentence_embeddings(model, batch))\n",
        "    Y.extend([1] * len(batch[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "uDGKJWJuXUdS"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "XAtOdcasXUdS"
      },
      "outputs": [],
      "source": [
        "X = np.concatenate(X)\n",
        "Y = np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "YvYAoKaJ4FOr"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQ-OG9zXUdS"
      },
      "outputs": [],
      "source": [
        "dataset_classifier = LogisticRegression(max_iter=1000)\n",
        "dataset_classifier.fit(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wJshnNlXUdS"
      },
      "outputs": [],
      "source": [
        "wnut_test_scores = []\n",
        "\n",
        "wnut_test_dataloader = torch.utils.data.DataLoader(wnut[\"test\"], batch_size=32, collate_fn=PadSequence(['input_ids', 'token_type_ids', 'attention_mask', 'labels']))\n",
        "for batch in tqdm.tqdm_notebook(wnut_test_dataloader):\n",
        "    x = get_sentence_embeddings(model, batch)\n",
        "    wnut_test_scores.append(dataset_classifier.predict_proba(x)[:,1])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "wRbozM7YXUdT"
      },
      "outputs": [],
      "source": [
        "wnut_test_scores = np.concatenate(wnut_test_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoaHuj71XUdT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "O_pTo_ofXUdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ef290900-f220-43f9-b116-6c36ec3d52ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(wnut_test_scores)\n",
        "plt.xlabel(\"WNUT score.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "NoR18YWmXUdT"
      },
      "outputs": [],
      "source": [
        "score_indices = np.argsort(wnut_test_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNwhUlH9XUdT"
      },
      "outputs": [],
      "source": [
        "wnut_predicted_labels = np.array(predicted_labels[\"wnut_test\"])[np.argsort(wnut_test_scores)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbcdK5VjXUdT"
      },
      "outputs": [],
      "source": [
        "wnut_true_labels = np.array([list(example[\"text_labels\"]) for example in wnut[\"test\"]])[np.argsort(wnut_test_scores)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp6gCKapXUdU"
      },
      "source": [
        "After sorting true and predicted labels based on WNUT score let split them into 5 chunks and measure F1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "0j-uXwh0XUdU"
      },
      "outputs": [],
      "source": [
        "predicted_splits = np.array_split(wnut_predicted_labels, 5, )\n",
        "true_splits = np.array_split(wnut_true_labels, 5)\n",
        "score_splits = np.array_split(wnut_test_scores[np.argsort(wnut_test_scores)], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "6G9vMWbNXUdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbc8715-aaa7-458e-a9c7-da0b82994546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score\tf1\n",
            "0.055\t0.453\n",
            "0.688\t0.319\n",
            "0.985\t0.308\n",
            "0.999\t0.177\n",
            "1.000\t0.152\n"
          ]
        }
      ],
      "source": [
        "print(\"score\\tf1\")\n",
        "for scores, true_split, predicted_split in zip(score_splits, true_splits, predicted_splits):\n",
        "    mean_score = np.mean(scores)\n",
        "    f1 = seqeval.metrics.f1_score(true_split, predicted_split)\n",
        "    print(f\"{mean_score:.3f}\\t{f1:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9k4P3bIXUdU"
      },
      "source": [
        "So:\n",
        "\n",
        "**The bigger domain shift the lower f1 measure**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BCiVCstC8AL3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "5_homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}